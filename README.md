# 🚀 `Kernel Coder`: GRPO-Trained Triton Kernel Model

An AI code assistant that helps write Triton kernel code.

We design an RL training pipeline to train a base model for generating [Triton Kernel](https://openai.com/index/triton/) code. Triton is a Python-based DSL for GPU programming. Inspired by [DeepSeek-R1-Zero](https://arxiv.org/abs/2501.12948), we implement a [GRPO](https://arxiv.org/abs/2402.03300)-based RL pipeline to train a base model ([`Qwen2.5-Coder-3B`](https://huggingface.co/Qwen/Qwen2.5-Coder-3B)).

## 📑 Table of Contents

- [Introduction](#Introduction)
	- [Goal](#-goal)
	- [Reward Design](#️-reward-design)
	- [Evaluation](#-evaluation)
	- [Our Contributions](#-our-contributions)
	- [Next Steps](#-next-steps-post-june-1st)
- [Dataset](#-dataset)
- [Model](#-model)
- [RL Training](#-rl-training)
- [Task](#task)
- [Evaluation Results](#evaluation-results)
- [Code Structure](#code-structure)
	- [How To Run](#-how-to-run)
- [Acknowledgements](#acknowledgements)

## Introduction

### 🎯 Goal: RL-train the Qwen2.5-Coder-3B base model on a Triton kernel dataset ([KernelBook](https://huggingface.co/datasets/GPUMODE/KernelBook)), aiming for competitive performance compared to the SFT-trained [KerneLllm](https://huggingface.co/facebook/KernelLLM) (based on [`Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B)).

### 🏗️ Reward Design

We design the reward function with two components:

1. ✅ Format Checking: Validate correct usage of `<thinking>` and `<answer>` tags.
2.	🔍 Similarity Score: Measure string similarity between generated and ground-truth Triton kernels using Python’s `difflib.SequenceMatcher`. This idea is inspired by [`SWE-RL`](https://arxiv.org/abs/2502.18449).

### 🧪 Evaluation

We evaluate the generated Triton kernels using [KernelBench](https://github.com/ScalingIntelligence/KernelBench.git) (`triton_backend_v2` branch) on:

- The base model (`Qwen2.5-Coder-3B`)
- The SFT model (`KernelLLM`)
- `kernel-coder` (our model): we will evaluate once the training is complete

### 🌟 Our Contributions

-  🎓 DeepSeek R1-Zero style **RL pipeline for Triton kernel** generation
-  📊 Reward model design: Combining format and **similarity-based rewards**


### 🔭 Next Steps (Post June 1st)

-	🧪 Add verifiable rewards: Use `KernelBench` to check compilation, correctness, and speedup.
-	🔄 Explore knowledge distillation: Distill [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) into a smaller model before applying RL training, then compare with our RL-trained model.


## 📦 Dataset ([KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook))

- ~18K samples of PyTorch kernels and corresponding Triton kernels (generated by Torch Inductor).
- Used for SFT training of [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM) and our GRPO training of `kernel-coder`.


## 🧠 Model

We apply GRPO training to `Qwen2.5-Coder-3B`, a compact yet strong code model from the Qwen 2.5 family, balancing performance and compute cost.


## 🔄 RL Training

Group Relative Policy Optimization (GRPO), proposed by DeepSeek, uses rule-based rewards for math and code tasks. GRPO avoids using a value model, instead estimating the advantage from relative reward rankings across multiple rollouts:

$$
\begin{aligned}
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
\end{aligned}
$$

This approach improves efficiency by comparing rollout quality relative to the batch.


## 🧪 Task

Generate Triton kernels equivalent or superior to provided PyTorch kernels.


## 📊 Evaluation Results

Using [KernelBench (Cuda and Triton kernel benchmark), (`triton_backend_v2` branch) ](https://github.com/ScalingIntelligence/KernelBench.git) to evaluate:

| Model | Compilation Rate (%) | Correctness Rate (%) |
|-------|---------------------|---------------------|
| KernelLLM | 77.0% | 12.0% |
| Qwen2.5-Coder-3B (untrained) | 29.0% | 3.0% |
| kernel-coder (ours, GRPO-trained) | 🚧 TBD | 🚧 TBD |

We test on label 1 (100 test cases) with temperature 1.0 and top_p 0.97. Preliminary results show the importance of Triton-specific training for compilation and correctness.

## 📂 Code Structure

The codebase consists of two main components:
1. `nano_r1_script.py` - Modified from [nano-aha-moment](https://github.com/McGill-NLP/nano-aha-moment/blob/f6384878831796fc29f560016e3cd570d264b823/nano_r1_script.py) for our `kernel-coder` project
2. [`KernelBench`](https://github.com/insop/KernelBench) - Forked and modified from [ScalingIntelligence/KernelBench](https://github.com/ScalingIntelligence/KernelBench)

Project structure:

```
kernel-coder/
├── README.md
├── kernel-coder
│   ├── nano_r1_script.py # main code
│   └── utils.py
└── scripts
    └── kernelllm.py # helper script from KernelLLM model, https://huggingface.co/facebook/KernelLLM
```

### 🏃 How To Run

```bash
cd kernel-coder # cd to the project root
python kernel-coder/nano_r1_script.py --nproc 8  --max_response_tokens 2048

```


## 🙏 Acknowledgements

We build on the following resources:

1. 📊 [KernelBench (Cuda and Triton kernel benchmark)](https://github.com/ScalingIntelligence/KernelBench.git)
2. 🔥 [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM)
3. 📚 [KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook)
4. 🧪 [nano-aha-moment (simple GRPO pipeline)](https://github.com/McGill-NLP/nano-aha-moment)
