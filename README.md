# ğŸš€ `Kernel Coder`: GRPO-Trained Triton Kernel Model

An AI code assistant that helps write Triton kernel code.

We design an RL training pipeline to train a base model for generating [Triton Kernel](https://openai.com/index/triton/) code. Triton is a Python-based DSL for GPU programming. Inspired by [DeepSeek-R1-Zero](https://arxiv.org/abs/2501.12948), we implement a [GRPO](https://arxiv.org/abs/2402.03300)-based RL pipeline to train a base model ([`Qwen2.5-Coder-3B`](https://huggingface.co/Qwen/Qwen2.5-Coder-3B)).

## ğŸ“‘ Table of Contents

- [Introduction](#Introduction)
	- [Goal](#-goal)
	- [Reward Design](#ï¸-reward-design)
	- [Evaluation](#-evaluation)
	- [Our Contributions](#-our-contributions)
	- [Next Steps](#-next-steps-post-june-1st)
- [Dataset](#-dataset)
- [Model](#-model)
- [RL Training](#-rl-training)
- [Task](#task)
- [Evaluation Results](#evaluation-results)
- [Code Structure](#code-structure)
	- [How To Run](#-how-to-run)
- [Acknowledgements](#acknowledgements)

## Introduction

### ğŸ¯ Goal: RL-train the Qwen2.5-Coder-3B base model on a Triton kernel dataset ([KernelBook](https://huggingface.co/datasets/GPUMODE/KernelBook)), aiming for competitive performance compared to the SFT-trained [KerneLllm](https://huggingface.co/facebook/KernelLLM) (based on [`Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B)).

### ğŸ—ï¸ Reward Design

We design the reward function with two components:

1. âœ… Format Checking: Validate correct usage of `<thinking>` and `<answer>` tags.
2.	ğŸ” Similarity Score: Measure string similarity between generated and ground-truth Triton kernels using Pythonâ€™s `difflib.SequenceMatcher`. This idea is inspired by [`SWE-RL`](https://arxiv.org/abs/2502.18449).

### ğŸ§ª Evaluation

We evaluate the generated Triton kernels using [KernelBench](https://github.com/ScalingIntelligence/KernelBench.git) (`triton_backend_v2` branch) on:

- The base model (`Qwen2.5-Coder-3B`)
- The SFT model (`KernelLLM`)
- `kernel-coder` (our model): we will evaluate once the training is complete

### ğŸŒŸ Our Contributions

-  ğŸ“ DeepSeek R1-Zero style **RL pipeline for Triton kernel** generation
-  ğŸ“Š Reward model design: Combining format and **similarity-based rewards**


### ğŸ”­ Next Steps (Post June 1st)

-	ğŸ§ª Add verifiable rewards: Use `KernelBench` to check compilation, correctness, and speedup.
-	ğŸ”„ Explore knowledge distillation: Distill [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) into a smaller model before applying RL training, then compare with our RL-trained model.


## ğŸ“¦ Dataset ([KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook))

- ~18K samples of PyTorch kernels and corresponding Triton kernels (generated by Torch Inductor).
- Used for SFT training of [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM) and our GRPO training of `kernel-coder`.


## ğŸ§  Model

We apply GRPO training to `Qwen2.5-Coder-3B`, a compact yet strong code model from the Qwen 2.5 family, balancing performance and compute cost.


## ğŸ”„ RL Training

Group Relative Policy Optimization (GRPO), proposed by DeepSeek, uses rule-based rewards for math and code tasks. GRPO avoids using a value model, instead estimating the advantage from relative reward rankings across multiple rollouts:

$$
\begin{aligned}
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
\end{aligned}
$$

This approach improves efficiency by comparing rollout quality relative to the batch.


## ğŸ§ª Task

Generate Triton kernels equivalent or superior to provided PyTorch kernels.


## ğŸ“Š Evaluation Results

Using [KernelBench (Cuda and Triton kernel benchmark), (`triton_backend_v2` branch) ](https://github.com/ScalingIntelligence/KernelBench.git) to evaluate:

| Model | Compilation Rate (%) | Correctness Rate (%) |
|-------|---------------------|---------------------|
| KernelLLM | 77.0% | 12.0% |
| Qwen2.5-Coder-3B (untrained) | 29.0% | 3.0% |
| kernel-coder (ours, GRPO-trained) | ğŸš§ TBD | ğŸš§ TBD |

We test on label 1 (100 test cases) with temperature 1.0 and top_p 0.97. Preliminary results show the importance of Triton-specific training for compilation and correctness.

## ğŸ“‚ Code Structure

The codebase consists of two main components:
1. `nano_r1_script.py` - Modified from [nano-aha-moment](https://github.com/McGill-NLP/nano-aha-moment/blob/f6384878831796fc29f560016e3cd570d264b823/nano_r1_script.py) for our `kernel-coder` project
2. [`KernelBench`](https://github.com/insop/KernelBench) - Forked and modified from [ScalingIntelligence/KernelBench](https://github.com/ScalingIntelligence/KernelBench)

Project structure:

```
kernel-coder/
â”œâ”€â”€ README.md
â”œâ”€â”€ kernel-coder
â”‚   â”œâ”€â”€ nano_r1_script.py # main code
â”‚   â””â”€â”€ utils.py
â””â”€â”€ scripts
    â””â”€â”€ kernelllm.py # helper script from KernelLLM model, https://huggingface.co/facebook/KernelLLM
```

### ğŸƒ How To Run

```bash
cd kernel-coder # cd to the project root
python kernel-coder/nano_r1_script.py --nproc 8  --max_response_tokens 2048

```


## ğŸ™ Acknowledgements

We build on the following resources:

1. ğŸ“Š [KernelBench (Cuda and Triton kernel benchmark)](https://github.com/ScalingIntelligence/KernelBench.git)
2. ğŸ”¥ [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM)
3. ğŸ“š [KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook)
4. ğŸ§ª [nano-aha-moment (simple GRPO pipeline)](https://github.com/McGill-NLP/nano-aha-moment)
