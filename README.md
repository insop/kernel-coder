# `Kernel Coder`: GRPO train to build Triton Kernel model

AI code assistant helps to write Kernel code.

We design RL training pipeline to train a base model to generate [Triton Kernel](https://openai.com/index/triton/), Triton is a Python based DSL for program GPU. Similar to [DeepSeek-R1-Zero](https://arxiv.org/abs/2501.12948), we design [GRPO](https://arxiv.org/abs/2402.03300) based RL pipeline to a base model (`Qwen2.5-Coder-3B`).

Our goal is to RL train the `Qwen2.5-Coder-3B` base model with Triton kernel dataset so that it can perform well compared to SFT trained [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) model, which is baed on `Llama-3.1-8B`.

We design the reward with two components: i) format checking, and ii) similarity score with ground throught Triton kernel. For format checking will check the thinking tag (`<thinking> </thinking`) and answer tag (`<answer> </answer>`). For similarity score, we use `Pythonâ€™s difflib.SequenceMatcher` to compare the similarity between the generated Triton kernel and ground truth Triton kernel. This idea is inspired by [`SWE-RL`](https://arxiv.org/abs/2502.18449).

To evaluate the geneated Triton kernel, we use [KernelBench](https://huggingface.co/datasets/GPUMODE/KernelBook) (`triton_backend_v2` branch) to evaluate our base model (`Qwen2.5-Coder-3B`), and SFT trained model (`KerneLllm`).

**Our contribution:**

Here are our own contributions to this project:

1. Design DeepSeek R1-zero style **RL training pipeline for Triton Kernel** generation LLM
2. Apply **similarity reward score** in addition to format reward score for reward model design

**Next Step (post June 1st)**

There are several ideas that we will continue after the initial phase:

1. For verifiable reward design, we could add `KernelBench`'s execution checks which includes compilation, correctnss, and speedup checks.

2. We could perform knowledge distillation from [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) model to a smaller model first instead of directly RL train. We apply RL train to the distilled model. Then we can compare this model with our initial model.

## Dataset

[KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook) is a dataset that includes around 18k samples of PyTorch kernel and associated Triton kernel, which is generated by torch inductor.

This dataset is used to SFT train [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM) recently.

We use this dataset to GRPO train our `kernel-coder`.

## Model

Our approach is to GRPO train a base model similar to DeepSeek R1-zero. We selected `Qwen2.5-Coder-3B` base model, since Qwen-2.5 is a good performing model family and this is a variant for coding. We choose smaller size, `3B`, to trade-off between performance and compute budget.

## RL Training


Group Relative Policy Optimization (GRPO) is proposed by DeepSeek team, which uses rule-based reward for math and coding tasks. In GRPO, value model is dropped and use the estimation, which helped save the compute resource. We execute multiple rollouts, trom these rollouts, we estimate advantage function based on relative goodness of these responses.

$$
\begin{aligned}
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}
\end{aligned}
$$


Advantage of each rollout is gap between its reward wrt the mean reward of the response divided by std.

## Task

The main task for the language model is generate equivlent or better performing Triton kernel given PyTorch kernel.

## Evaluation

[KernelBench (Cuda and Triton kernel benchmark)](https://github.com/ScalingIntelligence/KernelBench.git) is a dataset and benchmark which includes PyTorch kernels in 3 levels of difficulties. KernelBench is originally designed to benchmark the model generates CUDA kernel, and it does not include ground truth CUDA kernel. Instead, the KernelBench includes test cases that can verify the generated CUDA kernel in terms of compiliation, correctness, and speedup.


[`triton_backend_v2` branch](https://github.com/ScalingIntelligence/KernelBench/tree/triton_backend_v2) of the KernelBench, which is not yet merged, supports verifying Triton kernel generation.

We use this branch to verify Triton kernel generation.

Based on our own measurement, here is the correctness score `pass@1`. The training of our RL training model, `kernel-coder` is in-progress, so we made it black until we measure the final result. This is measure from label 1, which includes 100 test cases. We use temperature 1.0 and `top_p` 0.97 for all models. `Qwen2.5-Coder-3B`, a smaller base model that is not trained with Triton datset, shows much lower success rates for both compilatoion and coorectness rate compared `KernelLLM`, which is larger and SFT trained with Triton dataset.


| Model | Compilation Rate (%) | Correctness Rate (%) |
|-------|-----------------|------------------|
| `KernelLLM` | 77.0% | 12.0% |
| `Qwen2.5-Coder-3B` | 29.0% | 3.0% |
| kernel-coder (GRPO training to `Qwen2.5-Coder-3B` base) | TBD | TBD |

## Code structure


## Acknowledgements

We use the following model, dataset, and benchmark, and modifid [nano-aha-moment (simple GRPO pipeline)](git@github.com:insop/nano-aha-moment.git) for our project.

1. [KernelBench (Cuda and Triton kernel benchmark)](https://github.com/ScalingIntelligence/KernelBench.git)
2. [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM)
3. [KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook)
4. [nano-aha-moment (simple GRPO pipeline)](git@github.com:insop/nano-aha-moment.git)
