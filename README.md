# ğŸš€ `Kernel Coder`: GRPO-Trained Triton Kernel Model

An AI code assistant that helps write Triton kernel code.

We design an RL training pipeline to train a base model for generating [Triton Kernel](https://openai.com/index/triton/) code. Triton is a Python-based DSL for GPU programming. Inspired by [DeepSeek-R1-Zero](https://arxiv.org/abs/2501.12948), we implement a [GRPO](https://arxiv.org/abs/2402.03300)-based RL pipeline to train a base model ([`Qwen2.5-Coder-3B`](https://huggingface.co/Qwen/Qwen2.5-Coder-3B)).

ğŸ¯ Goal: RL-train the Qwen2.5-Coder-3B base model on a Triton kernel dataset ([KernelBook](https://huggingface.co/datasets/GPUMODE/KernelBook)), aiming for competitive performance compared to the SFT-trained [KerneLllm](https://huggingface.co/facebook/KernelLLM) (based on [`Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B)).

### ğŸ—ï¸ Reward Design

We design the reward function with two components:
	1.	âœ… Format Checking: Validate correct usage of `<thinking>` and `<answer>` tags.
	2.	ğŸ” Similarity Score: Measure string similarity between generated and ground-truth Triton kernels using Pythonâ€™s `difflib.SequenceMatcher`. This idea is inspired by [`SWE-RL`](https://arxiv.org/abs/2502.18449).

### ğŸ§ª Evaluation

We evaluate the generated Triton kernels using [KernelBench](https://github.com/ScalingIntelligence/KernelBench.git) (`triton_backend_v2` branch) on:
	â€¢	The base model (`Qwen2.5-Coder-3B`)
	â€¢	The SFT model (`KernelLLM`)

### ğŸŒŸ Our Contributions
	â€¢	ğŸ“ DeepSeek R1-Zero style **RL pipeline for Triton kernel** generation
	â€¢	ğŸ“Š Reward model design: Combining format and **similarity-based rewards**


### ğŸ”­ Next Steps (Post June 1st)
	â€¢	ğŸ§ª Add verifiable rewards: Use `KernelBench` to check compilation, correctness, and speedup.
	â€¢	ğŸ”„ Explore knowledge distillation: Distill [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) into a smaller model before applying RL training, then compare with our RL-trained model.


## ğŸ“¦ Dataset ([KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook))
	â€¢	~18K samples of PyTorch kernels and corresponding Triton kernels (generated by Torch Inductor).
	â€¢	Used for SFT training of [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM) and our GRPO training of `kernel-coder`.


## ğŸ§  Model

We apply GRPO training to `Qwen2.5-Coder-3B`, a compact yet strong code model from the Qwen 2.5 family, balancing performance and compute cost.


## ğŸ”„ RL Training

Group Relative Policy Optimization (GRPO), proposed by DeepSeek, uses rule-based rewards for math and code tasks. GRPO avoids using a value model, instead estimating the advantage from relative reward rankings across multiple rollouts:

$$
\begin{aligned}
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
\end{aligned}
$$

This approach improves efficiency by comparing rollout quality relative to the batch.


## ğŸ§ª Task

Generate Triton kernels equivalent or superior to provided PyTorch kernels.


## ğŸ“Š Evaluation Results

Using [KernelBench (Cuda and Triton kernel benchmark), (`triton_backend_v2` branch) ](https://github.com/ScalingIntelligence/KernelBench.git) to evaluate:

Model	Compilation Rate (%)	Correctness Rate (%)
KernelLLM	77.0%	12.0%
Qwen2.5-Coder-3B (untrained)	29.0%	3.0%
kernel-coder (GRPO-trained)	ğŸš§ TBD	ğŸš§ TBD

We test on label 1 (100 test cases) with temperature 1.0 and top_p 0.97. Preliminary results show the importance of Triton-specific training for compilation and correctness.


## ğŸ“‚ Code Structure



## ğŸ™ Acknowledgements

We build on the following resources:
	â€¢	ğŸ“Š KernelBench (CUDA & Triton kernel benchmark)
	â€¢	ğŸ”¥ KernelLLM (SFT model on KernelBook dataset)
	â€¢	ğŸ“š KernelBook (Triton kernel dataset)
	â€¢	ğŸ§ª nano-aha-moment (GRPO pipeline baseline)
