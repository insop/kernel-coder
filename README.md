# 🚀 `Kernel Coder`: GRPO-Trained Triton Kernel Model

An AI code assistant that helps write Triton kernel code.

We design an RL training pipeline to train a base model for generating [Triton Kernel](https://openai.com/index/triton/) code. Triton is a Python-based DSL for GPU programming. Inspired by [DeepSeek-R1-Zero](https://arxiv.org/abs/2501.12948), we implement a [GRPO](https://arxiv.org/abs/2402.03300)-based RL pipeline to train a base model ([`Qwen2.5-Coder-3B`](https://huggingface.co/Qwen/Qwen2.5-Coder-3B)).

🎯 Goal: RL-train the Qwen2.5-Coder-3B base model on a Triton kernel dataset ([KernelBook](https://huggingface.co/datasets/GPUMODE/KernelBook)), aiming for competitive performance compared to the SFT-trained [KerneLllm](https://huggingface.co/facebook/KernelLLM) (based on [`Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B)).

### 🏗️ Reward Design

We design the reward function with two components:
	1.	✅ Format Checking: Validate correct usage of `<thinking>` and `<answer>` tags.
	2.	🔍 Similarity Score: Measure string similarity between generated and ground-truth Triton kernels using Python’s `difflib.SequenceMatcher`. This idea is inspired by [`SWE-RL`](https://arxiv.org/abs/2502.18449).

### 🧪 Evaluation

We evaluate the generated Triton kernels using [KernelBench](https://github.com/ScalingIntelligence/KernelBench.git) (`triton_backend_v2` branch) on:
	•	The base model (`Qwen2.5-Coder-3B`)
	•	The SFT model (`KernelLLM`)

### 🌟 Our Contributions
	•	🎓 DeepSeek R1-Zero style **RL pipeline for Triton kernel** generation
	•	📊 Reward model design: Combining format and **similarity-based rewards**


### 🔭 Next Steps (Post June 1st)
	•	🧪 Add verifiable rewards: Use `KernelBench` to check compilation, correctness, and speedup.
	•	🔄 Explore knowledge distillation: Distill [`KerneLllm`](https://huggingface.co/facebook/KernelLLM) into a smaller model before applying RL training, then compare with our RL-trained model.


## 📦 Dataset ([KernelBook (Triton Kernel Dataset)](https://huggingface.co/datasets/GPUMODE/KernelBook))
	•	~18K samples of PyTorch kernels and corresponding Triton kernels (generated by Torch Inductor).
	•	Used for SFT training of [KerneLllm (SFT model with KernelBook dataset)](https://huggingface.co/facebook/KernelLLM) and our GRPO training of `kernel-coder`.


## 🧠 Model

We apply GRPO training to `Qwen2.5-Coder-3B`, a compact yet strong code model from the Qwen 2.5 family, balancing performance and compute cost.


## 🔄 RL Training

Group Relative Policy Optimization (GRPO), proposed by DeepSeek, uses rule-based rewards for math and code tasks. GRPO avoids using a value model, instead estimating the advantage from relative reward rankings across multiple rollouts:

$$
\begin{aligned}
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
\end{aligned}
$$

This approach improves efficiency by comparing rollout quality relative to the batch.


## 🧪 Task

Generate Triton kernels equivalent or superior to provided PyTorch kernels.


## 📊 Evaluation Results

Using [KernelBench (Cuda and Triton kernel benchmark), (`triton_backend_v2` branch) ](https://github.com/ScalingIntelligence/KernelBench.git) to evaluate:

Model	Compilation Rate (%)	Correctness Rate (%)
KernelLLM	77.0%	12.0%
Qwen2.5-Coder-3B (untrained)	29.0%	3.0%
kernel-coder (GRPO-trained)	🚧 TBD	🚧 TBD

We test on label 1 (100 test cases) with temperature 1.0 and top_p 0.97. Preliminary results show the importance of Triton-specific training for compilation and correctness.


## 📂 Code Structure



## 🙏 Acknowledgements

We build on the following resources:
	•	📊 KernelBench (CUDA & Triton kernel benchmark)
	•	🔥 KernelLLM (SFT model on KernelBook dataset)
	•	📚 KernelBook (Triton kernel dataset)
	•	🧪 nano-aha-moment (GRPO pipeline baseline)
